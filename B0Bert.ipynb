{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "white-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infinite-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations\n",
    "from transformers import *\n",
    "import geffnet\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "selective-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "disabled-acrobat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is (34250, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DOUBLE FOAM TAPE</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Campur - Leher Kancing (DPT001-00) Batik karakter Alhadi</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>[train_2406599165, train_3342059966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                                                                                title  \\\n",
       "0                                                                           Paper Bag Victoria Secret   \n",
       "1                                        Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DOUBLE FOAM TAPE   \n",
       "2                                                         Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Campur - Leher Kancing (DPT001-00) Batik karakter Alhadi   \n",
       "4                                                                   Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "\n",
       "   label_group                                target  \n",
       "0    249114794   [train_129225211, train_2278313361]  \n",
       "1   2937985045  [train_3386243561, train_3423213080]  \n",
       "2   2395904891  [train_2288590299, train_3803689425]  \n",
       "3   4093212188  [train_2406599165, train_3342059966]  \n",
       "4   3648931069   [train_3369186413, train_921438619]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(PATH/'train.csv')\n",
    "tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "train['target'] = train.label_group.map(tmp)\n",
    "print('train shape is', train.shape )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "loaded-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f1score(row, col):\n",
    "    n = len( np.intersect1d(row.target,row[col]) )\n",
    "    return 2*n / (len(row.target)+len(row[col]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-writer",
   "metadata": {},
   "source": [
    "## B0+Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regular-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(img_size=256):\n",
    "    return  albumentations.Compose([\n",
    "                albumentations.Resize(img_size, img_size),\n",
    "                albumentations.Normalize()\n",
    "            ])\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, csv, transforms=get_transforms(img_size=256), tokenizer=None):\n",
    "\n",
    "        self.csv = csv.reset_index()\n",
    "        self.transform = transforms\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "        \n",
    "        text = row.title\n",
    "        \n",
    "        image = cv2.imread(row.filepath)\n",
    "        image = image[:, :, ::-1]\n",
    "        \n",
    "        res0 = self.transform(image=image)\n",
    "        image0 = res0['image'].astype(np.float32)\n",
    "        image = image0.transpose(2, 0, 1)        \n",
    "\n",
    "        text = self.tokenizer(text, padding='max_length', truncation=True, max_length=16, return_tensors=\"pt\")\n",
    "        input_ids = text['input_ids'][0]\n",
    "        attention_mask = text['attention_mask'][0]\n",
    "\n",
    "        return torch.tensor(image), input_ids, attention_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tender-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PATH/'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enclosed-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34250 (tensor([[[ 0.6563,  0.4508,  0.4679,  ...,  0.3309,  0.3652,  0.4508],\n",
      "         [ 0.0741,  0.0227,  0.0227,  ...,  0.3652,  0.3994,  0.3994],\n",
      "         [ 0.4337,  0.5193,  0.5022,  ..., -0.1657,  0.1083,  0.3309],\n",
      "         ...,\n",
      "         [ 1.1529,  0.6049,  0.7419,  ...,  1.1700,  1.1872,  1.1872],\n",
      "         [ 0.7077,  0.8104,  0.7762,  ...,  1.1187,  1.3242,  1.1187],\n",
      "         [ 0.5193,  0.8789,  0.8104,  ...,  1.0844,  1.3242,  1.1015]],\n",
      "\n",
      "        [[-0.4251, -0.6352, -0.6176,  ..., -0.3901, -0.5126, -0.5476],\n",
      "         [-1.0203, -1.0728, -1.0728,  ..., -0.2500, -0.3725, -0.4951],\n",
      "         [-0.6527, -0.5651, -0.5826,  ..., -0.4776, -0.3725, -0.2850],\n",
      "         ...,\n",
      "         [ 0.2577, -0.4951, -0.2850,  ..., -0.0399,  0.3102,  0.1176],\n",
      "         [-0.3025, -0.2500, -0.1625,  ..., -0.1450,  0.3452,  0.0301],\n",
      "         [-0.4601, -0.0924, -0.1275,  ..., -0.1975,  0.2752,  0.0126]],\n",
      "\n",
      "        [[-0.0267, -0.2358, -0.2184,  ...,  0.0431, -0.0615, -0.1138],\n",
      "         [-0.6193, -0.6715, -0.6715,  ...,  0.2871,  0.1825,  0.0605],\n",
      "         [-0.2532, -0.1661, -0.1835,  ..., -0.0441,  0.1476,  0.2871],\n",
      "         ...,\n",
      "         [ 0.7925,  0.1128,  0.2871,  ...,  0.5659,  0.7925,  0.6705],\n",
      "         [ 0.2696,  0.3393,  0.3742,  ...,  0.4788,  0.8622,  0.5834],\n",
      "         [ 0.0953,  0.4788,  0.4265,  ...,  0.4265,  0.8274,  0.5834]]]), tensor([ 101, 3259, 4524, 3848, 3595,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "df_test = train.copy()\n",
    "df_test['filepath'] = df_test['image'].apply(lambda x: str(PATH/'train_images'/x))\n",
    "dataset_test = ImageTextDataset(df_test, transforms=get_transforms(img_size=256), tokenizer=tokenizer)\n",
    "test_loader = DataLoader(dataset_test, batch_size=64, num_workers=16)\n",
    "\n",
    "print(len(dataset_test),dataset_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "trained-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k=3):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
    "        self.k = k\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
    "        cosine, _ = torch.max(cosine_all, dim=2)\n",
    "        return cosine \n",
    "    \n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "class Swish(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "class Swish_module(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return Swish.apply(x)\n",
    "\n",
    "    \n",
    " \n",
    "    \n",
    "class enet_arcface_FINAL(nn.Module):\n",
    "\n",
    "    def __init__(self, enet_type, out_dim):\n",
    "        super(enet_arcface_FINAL, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PATH/'bert-base-uncased')\n",
    "        self.enet = geffnet.create_model(enet_type.replace('-', '_'), pretrained=None)\n",
    "        self.feat = nn.Linear(self.enet.classifier.in_features+self.bert.config.hidden_size, 512)\n",
    "        self.swish = Swish_module()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.metric_classify = ArcMarginProduct_subcenter(512, out_dim)\n",
    "        self.enet.classifier = nn.Identity()\n",
    " \n",
    "    def forward(self, x,input_ids, attention_mask):\n",
    "        x = self.enet(x)\n",
    "        text = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "        x = torch.cat([x, text], 1)\n",
    "        x = self.swish(self.feat(x))\n",
    "        return F.normalize(x), self.metric_classify(x)\n",
    "    \n",
    "def load_model(model, model_file):\n",
    "    state_dict = torch.load(model_file)\n",
    "    if \"model_state_dict\" in state_dict.keys():\n",
    "        state_dict = state_dict[\"model_state_dict\"]\n",
    "    state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n",
    "#     del state_dict['metric_classify.weight']\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(f\"loaded {model_file}\")\n",
    "    model.eval()    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suspended-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "WGT = PATH/'b0ns_256_bert_20ep_fold0_epoch27.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blond-seafood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/slex/data/shopee/b0ns_256_bert_20ep_fold0_epoch27.pth\n"
     ]
    }
   ],
   "source": [
    "model = enet_arcface_FINAL('tf_efficientnet_b0_ns', out_dim=11014).cuda()\n",
    "model = load_model(model, WGT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "french-respect",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536/536 [00:34<00:00, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 4.83 s, total: 32.4 s\n",
      "Wall time: 34.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, input_ids, attention_mask in tqdm(test_loader): \n",
    "        img, input_ids, attention_mask = img.cuda(), input_ids.cuda(), attention_mask.cuda()\n",
    "        feat, _ = model(img, input_ids, attention_mask)\n",
    "        image_embeddings = feat.half()\n",
    "        embeds.append(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sorted-gazette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image embeddings shape torch.Size([34250, 512])\n"
     ]
    }
   ],
   "source": [
    "image_embeddings = torch.cat(embeds)\n",
    "print('image embeddings shape',image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-briefing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "extended-harassment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.31 s, sys: 20.1 ms, total: 3.33 s\n",
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds=[[] for _ in range(len(df_test))]\n",
    "\n",
    "CHUNK=10000\n",
    "for start in range(0, len(df_test), CHUNK):\n",
    "    cos_sim = image_embeddings[start:start+CHUNK] @ image_embeddings.T\n",
    "    idxa, idxb =torch.where(cos_sim>.5)\n",
    "    dfb=df_test.iloc[idxb.cpu()].posting_id.values\n",
    "    for a,b in zip(idxa, dfb):\n",
    "        preds[a+start].append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "simplified-blowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for baseline = 0.9088021070935636\n"
     ]
    }
   ],
   "source": [
    "df_test['preds_b0bert']=preds\n",
    "\n",
    "df_test['b0bert_score'] = df_test.apply(functools.partial(f1score, col='preds_b0bert'),axis=1)\n",
    "print('CV score for baseline =',df_test.b0bert_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "metropolitan-growing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_129225211 train_2278313361',\n",
       " 'train_3386243561 train_3423213080',\n",
       " 'train_2288590299 train_3803689425',\n",
       " 'train_2406599165',\n",
       " 'train_3369186413 train_921438619']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [' '.join(p) for p in preds]\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cathedral-cabin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_129225211 train_2278313361'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-malpractice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python38564bitfastaicondad52d12c5a30a4725bf9d3e235cf1271c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
